==================================
Advanced example to using GPry
==================================

This example shows some of the ways in which it is possible to customize the
Bayesian optimization loop. This will be done using a less standard
(non-gaussian) likelihood and we will walk through the modules and their
functionalities one by one.

The Function
============

In this example we use a curved 2d variation of a guassian with the curving
degeneracy being generated by a 8-degree polynomial in the exponent.
It has the following likelihood::

.. math::
    y(x) \sim \exp(-25\cdot(0.45-x_1)^2 - 20\cdot(x_2/2-x_1^4)^2)

with :math:`x=(x_1, x_2)^T`.

Since our GP maps the log-posterior and uses the log-likelihood as input the
likelihood function looks something like this::

    def log_lkl(x_1, x_2):
        return  -(10*(0.45-x_1))**2./4. - (20*(x_2/4.-x_1**4.))**2.

We could map this likelihood using the standard parameters of the
:meth:`run.run` function, we will change them however since the goal of this
example is to showcase the ways in which the BO loop can be modified.
Furthermore the individual modules contain instructions on how to implement
the different custom options as well as on how to create custom classes which
inherit from the base classes.

The Model
=========
It is generally a good idea to define the model (i.e. the prior) first. For
this we need to create a Cobaya model object which contains the prior and
log-likelihood::

    from cobaya.model import get_model
    info = {"likelihood": {"curved_degeneracy": log_lkl}}
    info["params"] = {
        "x_1": {"prior": {"min": -0.5, "max": 1.5}},
        "x_2": {"prior": {"min": -0.5, "max": 2.}}
        }
    model = get_model(info)

For building the parts of our BO-loop we will need the dimensionality and the
prior bounds of our model which we get in the following way::

    dim = model.prior.d()
    prior_bounds = model.prior.bounds()

If you have unbounded priors (like a normal distribution) use the keyword
``confidence_for_unbounded``
(see `here <https://cobaya.readthedocs.io/en/latest/params_prior.html#prior-class>`_)

The GP Regressor
================

Even though the standard RBF kernel would work well in this case we will spice
things up a bit by constructing a custom kernel which consists of a Mat√®rn
kernel with :math:`\nu=5/2` multiplied with a Constant kernel (and non-dynamic,
i.e. fixed bounds). Also we will increase ``n_restarts_optimizer`` (the number
of restarts of the optimizer for the GP's hyperparameters) to 20.
Furthermore it is generally a good idea to scale the parameter space to make it
a unit hypercube::

    from gpry.preprocessing import Normalize_bounds
    from gpry.gpr import GaussianProcessRegressor
    from gpry.kernels import Matern, ConstantKernel as C
    from gpry.gp_acquisition import GP_Acquisition
    from gpry.acquisition_functions import Expected_improvement

    normalize_bounds = Normalize_bounds(prior_bounds)
  	kernel = C(1.0, (1e-3, 1e3)) * Matern([1.0]*2, [[1e-5, 1e5]*2], nu=2.5)
  	gp = GaussianProcessRegressor(kernel=kernel,
  		                     n_restarts_optimizer=20,
                           normalize_X=normalize_bounds)
  	af = Log_exp(zeta=0.1)

In case you wonder what these mean please refer to the :mod:`kernels`,
:mod:`gpr` and :mod:`preprocessing` modules.

Acquisition
===========

and :mod:`acquisition_functions` modules.

Then it is time for the actual GP Acquisition. For this we need to
build our instance of the :class:`gp_acquisition.GP_Acquisition` class.
In our case we need some prior bounds (for our uniform prior)::

    bnds = np.array([[-10.,10.], [-10.,10.]])
    acquire = GP_Acquisition(bnds,
    			      acq_func=af,
                             n_restarts_optimizer=20)

Preprocessing the data for the GP regressor and the acquisition
module will be discussed in the advanced example

.. note::
    In our example we set ``n_restarts_optimizer`` to quite a high value.
    In most applications it wouldn't have to be set this high, thus saving
    a lot of computation time.

Training
========

We start by random-generating 3 initial points from which to start
our exploration of the function::

    init_1 = np.random.uniform(bnds[0,0], bnds[0,1], 3)
    init_2 = np.random.uniform(bnds[1,0], bnds[1,1], 3)


    init_X = np.stack((init_1, init_2), axis=1)
    init_y = f(init_X)

    gp.append_to_data(init_X, init_y, fit=True)

.. note::
    The part where initial values are drawn and fit to the GP will
    be automated later.

Now it is time to train our model. We will do this manually with
a loop::

    n_points = 2
    for _ in range(5):
        new_X, y_lies, acq_vals = acquire.multi_optimization(n_points=n_points)
        new_y = f(new_X)
        acquire.surrogate_model.append_to_data(new_X, new_y)

Let us look at this step by step:

    * First we specify how many points shall be
      acquired per step (here it's 2)
    * We want to do 5 acquisition runs (therefore the ``range(5)``)
    * The :meth:`acquire.multi_optimization` method optimizes the
      acquisition function and returns the 2 points to query ``new_X``
      as well as the "fake" values of the surrogate model at these points.
    * The next line calls the real values of the function
    * These new values are appended to the training points of the model
      nested inside the :class:`gp_acquisition.GP_Acquisition` object.

Let us now see how the model has performed by plotting the GP prediction
(again we plot the negative prediction because of the log-scale)::

    # Getting the prediction
    gp = acquire.surrogate_model
    x_gp = gp.X_train_[:,0]
    y_gp = gp.X_train_[:,1]
    y_fit, std_fit = gp.predict(x, return_std=True)
    y_fit = -1 * y_fit.reshape(xdim[:-1])

    # Plot surrogate
    fig = plt.figure()
    im = plt.pcolor(A, B, y_fit, norm=LogNorm())
    plt.scatter(x_gp[:5], y_gp[:5], color="purple")
    plt.scatter(x_gp[5:], y_gp[5:], color="black")
    plt.xlabel(r"$x$")
    plt.ylabel(r"$y$")
    plt.xlim((-10, 10))
    plt.ylim((-10, 10))
    fig.subplots_adjust(right=0.8)
    cbar_ax = fig.add_axes([0.85, 0.1, 0.05, 0.8])
    cbar = fig.colorbar(im, cax=cbar_ax, orientation='vertical')

.. image:: images/Surrogate.png
   :width: 600

Here the purple dots are the initial samples we drew randomly while the black
dots are acquired points. The red dot (barely visible) is the real minimum.

Plotting with Cobaya
====================

Let us now compare triangle plots generated by Cobaya with

 #. The actual function
 #. The surrogate model

 For this we first need to import the modules::

    from cobaya.run import run
    from getdist.mcsamples import MCSamplesFromCobaya
    import getdist.plots as gdplt

1. Actual function
******************

Since the true function (and thus also the surrogate model) are defined
on the log-likelihood we can just go ahead and define a function which Cobaya
understands. This means basically just copying from the Cobaya examples::

    def true_func(x,y):
        return f(np.array([[x,y]]))

    info = {"likelihood": {"true_func": true_func}}
    info["params"] = {
        "x": {"prior": {"min": -10, "max": 10}, "ref": 0.5, "proposal": 0.2},
        "y": {"prior": {"min": -10, "max": 10}, "ref": 0.5, "proposal": 0.2}}

    info["sampler"] = {"mcmc": {"Rminus1_stop": 0.001, "max_tries": 1000}}

    updated_info, sampler = run(info)

    gdsamples_mcmc = MCSamplesFromCobaya(updated_info, sampler.products()["sample"])
    gdplot = gdplt.get_subplot_plotter(width_inch=5)
    gdplot.triangle_plot(gdsamples_mcmc, ["x", "y"], filled=True)

.. image:: images/Ground_truth_triangle.png
   :width: 600

.. note::

    We set the precision parameters (specifically ``Rminus1_stop``) to be very
    accurate. In most examples a value of 0.005-0.01 would be enough.

2. Surrogate model
******************

For comparison we produce a triangle plot of the surrogate model
(Again with Cobaya)::

    def callonmodel(x,y):
        return gp.predict(np.array([[x,y]]))

    info = {"likelihood": {"gpsurrogate": callonmodel}}
    info["params"] = {
        "x": {"prior": {"min": -10, "max": 10}, "ref": 0.5, "proposal": 0.2},
        "y": {"prior": {"min": -10, "max": 10}, "ref": 0.5, "proposal": 0.2}}

    info["sampler"] = {"mcmc": {"Rminus1_stop": 0.001, "max_tries": 1000}}

    updated_info, sampler = run(info)

    gdsamples_gp = MCSamplesFromCobaya(updated_info, sampler.products()["sample"])
    gdplot = gdplt.get_subplot_plotter(width_inch=5)
    gdplot.triangle_plot(gdsamples_gp, ["x", "y"], filled=True)

.. image:: images/Surrogate_triangle.png
   :width: 600

Now we can compare the two to see if our GP finds the same contours as the MCMC::

    gdplot = gdplt.get_subplot_plotter(width_inch=5)
    gdplot.triangle_plot([gdsamples_mcmc, gdsamples_gp], ["x", "y"], filled=True,
        legend_labels=['MCMC', 'GP'])

.. image:: images/Comparison_triangle.png
   :width: 600

As you can see the two agree almost perfectly! And we achieved this with just 13
evaluations of the Posterior distribution!

The code for the example is available at :download:`../../examples/simple_example.py`

==================================
Advanced example to using GPry
==================================

This example shows some of the ways in which it is possible to customize the
Bayesian optimization loop. This will be done using a less standard
(non-gaussian) likelihood and we will walk through the modules and their
functionalities one by one.

Introduction
============

In this example we use a curved 2d variation of a guassian with the curving
degeneracy being generated by a 8-degree polynomial in the exponent.
It has the following (un-normalized) likelihood:

.. math::
    y(x) \sim \exp(-25\cdot(0.45-x_1)^2 - 20\cdot(x_2/2-x_1^4)^2)

with :math:`x=(x_1, x_2)^T`.

Since our GP maps the log-posterior and uses the log-likelihood as input the
likelihood function looks something like this::

    def log_lkl(x_1, x_2):
        return  -(10*(0.45-x_1))**2./4. - (20*(x_2/4.-x_1**4.))**2.

We could map this likelihood using the standard parameters of the
:meth:`run.run` function, which would recover the correct marginals.
Since the goal of this example is to showcase the ways in which the BO loop
can be modified we will change things though.

In order to make this as easy to understand as possible I will walk you through
every step of building the model, running it and plotting. I will reference the
individual modules where they are used. They contain further detailed
documentation on how to implement the different custom options as well as on
how to create custom classes which inherit from the base classes.

The GPry algorithm needs 5 basic building blocks to learn the shape of a
posterior distribution:

* The **model** which contains the log-likelihood and the prior.
* The **GP regressor** which is used to interpolate the posterior.
* The **Acquisition** object which determines the next sampling locations in
  each step of the Bayesian optimization loop.
* The **Convergence criterion** which determines when the GP has converged to
  the shape of the posterior distribution and and hence the Bayesian
  optimization loop should be stopped.
* The **options** dictionary setting the parameters for the actualy Bayesian
  optimization.

This is followed by a call to the run function which 

The Model
=========
It is generally a good idea to define the model (i.e. the prior) first. For
this we need to create a Cobaya model object which contains the prior and
log-likelihood::

    from cobaya.model import get_model
    info = {"likelihood": {"curved_degeneracy": log_lkl}}
    info["params"] = {
        "x_1": {"prior": {"min": -0.5, "max": 1.5}},
        "x_2": {"prior": {"min": -0.5, "max": 2.}}
        }
    model = get_model(info)

For building the parts of our BO-loop we will need the dimensionality and the
prior bounds of our model which we get in the following way::

    dim = model.prior.d()
    prior_bounds = model.prior.bounds()

If you have unbounded priors (like a normal distribution) use the keyword
``confidence_for_unbounded``
(see `here <https://cobaya.readthedocs.io/en/latest/params_prior.html#prior-class>`_)

The GP Regressor
================

The first part of our model which we have to define is the GP regressor. This
object also contains the kernel. It's hyperparameters are optimized in every
iteration of the BO loop.

Kernel
""""""

Even though the standard RBF kernel would work well enough for this likelihood
we will spice things up a bit by constructing a custom kernel which consists of
a Matèrn kernel with :math:`\nu=5/2` multiplied with a Constant kernel (and
non-dynamic, i.e. fixed bounds)::

    from gpry.kernels import Matern, ConstantKernel as C

    kernel = C(1.0, (1e-3, 1e3)) * Matern([0.1]*dim, [[1e-5, 1e5]]*dim, nu=2.5)

For details on the kernel construction see :mod:`kernels`.

.. note::
    For kernels containing length-scales (i.e. RBF and Matèrn kernel) the
    bounds can automatically be adjusted according to the size of the prior.

GP Regressor
""""""""""""

Now it's time to construct the actual GP regressor object. In addition to the
kernel this sets all the variables associated to the optimization procedure of
the hyperparameters as well as how the data is preprocessed.
Since we want our model to converge to the correct hyperparameters more robustly
we increase ``n_restarts_optimizer``(the number of restarts of the optimizer
for the GP's hyperparameters) to 20.

Furthermore it is generally a good idea to scale the parameter space to make it
a unit hypercube as is done as standard when calling the :meth:`run.run`
function::

    from gpry.gpr import GaussianProcessRegressor
    from gpry.preprocessing import Normalize_bounds

    gpr = GaussianProcessRegressor(kernel=kernel,
    	                           n_restarts_optimizer=20,
                                   preprocessing_X=Normalize_bounds(prior_bounds))

Details can be found in the :mod:`gpr` and :mod:`preprocessing` modules.

.. note::
    The SVM which divides posterior samples into a *finite* and an *infinite*
    category is part of the GP regressor. I advise keeping it as standard but
    there are options for changing it. For this see the :mod:`svm` module.

.. note::
    We did not process the target values of the posterior distribution before
    fitting the GP. In our example this is not a bit problem as the range of
    the log-likelihood is relatively modest. If your log-likelihood ranges
    several orders of magnitude (i.e. when you have a big prior) it is usually
    a good idea to scale your target values using :class:`preprocessing.Normalize_y`

Acquisition
===========

The acquisition module contains both the acquisition function as well as the
optimization procedure for it. It operates similarly to the GP regressor module.

Acquisition function
""""""""""""""""""""

The acquisition function is the centerpiece of the Bayesion optimization
procedure and decides which point the algorithm samples next. The
:mod:`acquisition_functions` module has multiple inbuilt acquisition functions
as well as building blocks for custom acquistion functions which can be
constructed using the + and * operators. Since it tends to perform best we will
use the standard :class:`acquisition_functions.Log_exp` acquisition function
with a :math:`\zeta` value of 0.05 to encourage exploration (as we know that
the shape of the posterior distribution is not very gaussian)::

  	af = Log_exp(zeta=0.05)

Then it is time for the actual GP Acquisition. For this we need to
build our instance of the :class:`gp_acquisition.GP_Acquisition` class which
also takes the acquisition function. Furthermore it needs the prior bounds
so it knows which volume to sample in. Furthermore like with the GP regressor
it is usually a good idea to scale the prior bounds to a unit hypercube
(assuming that the mode occupies roughly the same portion of the prior in each
dimension) as the optimizer tends to struggle with very different scales across
different dimensions::

    acquisition = GP_Acquisition(prior_bounds,
    			                   acq_func=af,
                                 preprocessing_X=Normalize_bounds(prior_bounds))

Convergence
===========

Next we need to set how the algorithm determines whether it has converged to
the correct posterior distribution. This is set using the :mod:`convergence`
module which offers a base :class:`convergence.ConvergenceCriterion` class
of which several inbuilt convergence criteria inherit. Using this base class
it is also possible to construct custom convergence criteria.
Here we will use the :class:`convergence.CorrectCounter` convergence criterion
which uses the accuracy of the GP at the next sampling location to assess
convergence::

    convergence =

Training options
================

Now that we have set all the

Training
========

We start by random-generating 3 initial points from which to start
our exploration of the function::

    init_1 = np.random.uniform(bnds[0,0], bnds[0,1], 3)
    init_2 = np.random.uniform(bnds[1,0], bnds[1,1], 3)


    init_X = np.stack((init_1, init_2), axis=1)
    init_y = f(init_X)

    gp.append_to_data(init_X, init_y, fit=True)

.. note::
    The part where initial values are drawn and fit to the GP will
    be automated later.

Now it is time to train our model. We will do this manually with
a loop::

    n_points = 2
    for _ in range(5):
        new_X, y_lies, acq_vals = acquire.multi_optimization(n_points=n_points)
        new_y = f(new_X)
        acquire.surrogate_model.append_to_data(new_X, new_y)

Let us look at this step by step:

    * First we specify how many points shall be
      acquired per step (here it's 2)
    * We want to do 5 acquisition runs (therefore the ``range(5)``)
    * The :meth:`acquire.multi_optimization` method optimizes the
      acquisition function and returns the 2 points to query ``new_X``
      as well as the "fake" values of the surrogate model at these points.
    * The next line calls the real values of the function
    * These new values are appended to the training points of the model
      nested inside the :class:`gp_acquisition.GP_Acquisition` object.

Let us now see how the model has performed by plotting the GP prediction
(again we plot the negative prediction because of the log-scale)::

    # Getting the prediction
    gp = acquire.surrogate_model
    x_gp = gp.X_train_[:,0]
    y_gp = gp.X_train_[:,1]
    y_fit, std_fit = gp.predict(x, return_std=True)
    y_fit = -1 * y_fit.reshape(xdim[:-1])

    # Plot surrogate
    fig = plt.figure()
    im = plt.pcolor(A, B, y_fit, norm=LogNorm())
    plt.scatter(x_gp[:5], y_gp[:5], color="purple")
    plt.scatter(x_gp[5:], y_gp[5:], color="black")
    plt.xlabel(r"$x$")
    plt.ylabel(r"$y$")
    plt.xlim((-10, 10))
    plt.ylim((-10, 10))
    fig.subplots_adjust(right=0.8)
    cbar_ax = fig.add_axes([0.85, 0.1, 0.05, 0.8])
    cbar = fig.colorbar(im, cax=cbar_ax, orientation='vertical')

.. image:: images/Surrogate.png
   :width: 600

Here the purple dots are the initial samples we drew randomly while the black
dots are acquired points. The red dot (barely visible) is the real minimum.

Plotting with Cobaya
====================

Let us now compare triangle plots generated by Cobaya with

 #. The actual function
 #. The surrogate model

 For this we first need to import the modules::

    from cobaya.run import run
    from getdist.mcsamples import MCSamplesFromCobaya
    import getdist.plots as gdplt

1. Actual function
******************

Since the true function (and thus also the surrogate model) are defined
on the log-likelihood we can just go ahead and define a function which Cobaya
understands. This means basically just copying from the Cobaya examples::

    def true_func(x,y):
        return f(np.array([[x,y]]))

    info = {"likelihood": {"true_func": true_func}}
    info["params"] = {
        "x": {"prior": {"min": -10, "max": 10}, "ref": 0.5, "proposal": 0.2},
        "y": {"prior": {"min": -10, "max": 10}, "ref": 0.5, "proposal": 0.2}}

    info["sampler"] = {"mcmc": {"Rminus1_stop": 0.001, "max_tries": 1000}}

    updated_info, sampler = run(info)

    gdsamples_mcmc = MCSamplesFromCobaya(updated_info, sampler.products()["sample"])
    gdplot = gdplt.get_subplot_plotter(width_inch=5)
    gdplot.triangle_plot(gdsamples_mcmc, ["x", "y"], filled=True)

.. image:: images/Ground_truth_triangle.png
   :width: 600

.. note::

    We set the precision parameters (specifically ``Rminus1_stop``) to be very
    accurate. In most examples a value of 0.005-0.01 would be enough.

2. Surrogate model
******************

For comparison we produce a triangle plot of the surrogate model
(Again with Cobaya)::

    def callonmodel(x,y):
        return gp.predict(np.array([[x,y]]))

    info = {"likelihood": {"gpsurrogate": callonmodel}}
    info["params"] = {
        "x": {"prior": {"min": -10, "max": 10}, "ref": 0.5, "proposal": 0.2},
        "y": {"prior": {"min": -10, "max": 10}, "ref": 0.5, "proposal": 0.2}}

    info["sampler"] = {"mcmc": {"Rminus1_stop": 0.001, "max_tries": 1000}}

    updated_info, sampler = run(info)

    gdsamples_gp = MCSamplesFromCobaya(updated_info, sampler.products()["sample"])
    gdplot = gdplt.get_subplot_plotter(width_inch=5)
    gdplot.triangle_plot(gdsamples_gp, ["x", "y"], filled=True)

.. image:: images/Surrogate_triangle.png
   :width: 600

Now we can compare the two to see if our GP finds the same contours as the MCMC::

    gdplot = gdplt.get_subplot_plotter(width_inch=5)
    gdplot.triangle_plot([gdsamples_mcmc, gdsamples_gp], ["x", "y"], filled=True,
        legend_labels=['MCMC', 'GP'])

.. image:: images/Comparison_triangle.png
   :width: 600

As you can see the two agree almost perfectly! And we achieved this with just 13
evaluations of the Posterior distribution!

The code for the example is available at :download:`../../examples/simple_example.py`
